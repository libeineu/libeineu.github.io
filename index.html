<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bei Li</title>
  
  <meta name="author" content="Bei Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <!-- <p>[<a href="index_cn.html">‰∏≠Êñá‰∏ªÈ°µ</a>]</p>  -->
                <name>Bei Li</name>  
                
              </p>
              <p>I am a third year Ph.D student at the Department of <a href="http://www.cse.neu.edu.cn/">Computer Science and Technology</a> at <a href="http://www.neu.edu.cn/"> Northeastern University</a>, China. where I work at <a href="https://www.nlplab.com/">Natural Language Processing Lab </a> under the supervision of Prof. <a href="https://www.nlplab.com/members/xiaotong.html">Tong Xiao</a> and Prof. <a href="https://www.nlplab.com/members/zhujingbo.html"> Jingbo Zhu</a>.
              </p>
              <p>
                I received my bachelor degree in 2017 from Northeastern University, majoring in Computer Science and Technology, and my master in 2020 from Northeastern University, majoring in Computer Software and Theory.     
              </p>
              <p>
                I join the NEUNLP LAB at the fourth year of my college life. My research interest contains complex architecture modeling, deep Transformeer, multimodal modeling and machine learning. Welcome to contact me if you are interested in what I am doing or have questions to discuss!
              </p>
              <p style="text-align:center">
                <a href="libei_neu@outlook.com">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">Resume</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=wzbJ5EIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/libeineu/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/LiBei.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/libei.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Research</heading>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                I major in natural language processing, especially the sequence generation task, including machine translation, abstractive summarization and etc.
                My current focus majorly is to build parameter-efficient backbone for NLP. 

              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>News</heading>
            <ul>
              <li>
                [May'2023] <strong>Three</strong> papers (1 main and 2 Findings) accepted by <a href="https://2023.aclweb.org/">ACL 2023</a>.
              </li>
              <li>
                [Dec'2022] Finished my internship at NLC, and start a new internship at Machine Learning Group (ML)</a>.
              </li>
              <li>
                [May'2022] Started my internship at MicroSoft Research Asia<a href="https://www.msra.cn/">(MSRA)</a>, Natural Language Computing (NLC).
              </li>
               <li>
                [Apr'2022] <strong>One</strong> paper on learning multiscale Transformer models for sequence generation accepted by <a href="https://icml.cc/Conferences/2022">ICML 2022</a>.
              </li>
              <li>
                [Feb'2022] <strong>Two</strong> papers on parameter-efficient backbone and multimodal machine translation accepted by <a href="https://www.2022.aclweb.org/">ACL 2022</a>.
              </li> 
              <li>
                [Apr'2021] <strong>One</strong> paper on knowledge distillation accepted to <a href="https://2021.aclweb.org/">ACL 2021</a>.
              </li>
              <li>
                [Nov'2020] <strong>One</strong> paper on deep Transformer compression accepted to <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>.
              </li>
              <li>
                [Sep'2020] <strong>One</strong> paper on shallow-to-deep training for deep Transformer models accepted to <a href="https://2020.emnlp.org/">EMNLP 2020</a>.
              </li>
              <li>
                [Apr'2020] <strong>One</strong> paper on context-aware machine translation accepted to <a href="https://2020.aclweb.org/">ACL 2020</a>.
              </li>
              <li>
                [May'2019] <strong>One</strong> paper on the NiuTrans submission of WMT19 accepted to <a href="https://www.statmt.org/wmt19/">WMT 2019</a>.
              </li>
              <li>
                [May'2019] <strong>One</strong> paper on learning deep Transformer models accepted to <a href="https://2019.aclweb.org/">ACL 2019</a>.
              </li>
              

            </ul>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications</heading>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/Neurips_submission.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Deliberate then Generate: Enhanced Prompting Framework for Text Generation</papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong Xiao, Jiang Bian and JingBo Zhu
              <br>
              <!-- <em>61th Annual Meeting of the Association for Computational Linguistics (<strong>Findings of ACL</strong>)</em>, 2023  -->
              <em> In progress, comming soon. </em>
              <br>
							<!-- <a href="#">[pdf](not aviable)</a> / 
							<a href="#">[code](not aviable)</a>  -->
							<a href="">[pdf]</a> / [<a href="">code</a>]
              <p></p>
              <p>
                We encourage the model to deliberate by proposing a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors. DTG is a simple yet effective technique that can be applied to various text generation tasks with minimal modifications. We conduct extensive experiments on 20+ datasets across 7 text generation tasks, including summarization, translation, dialogue, and more. We show that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/ACL2023_manager.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle> Augmenting Large Language Model Translators via Translation Memories</papertitle>
              <!-- </a> -->
              <br>
              Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, <strong>Bei Li</strong>, Yinqiao Li, Tong Xiao, Chunliang Zhang and Jingbo Zhu
              <br>
              <em>61th Annual Meeting of the Association for Computational Linguistics (<strong>Findings of ACL</strong>)</em>, 2023 
              <br>
							<!-- <a href="#">[pdf](not aviable)</a> / 
							<a href="#">[code](not aviable)</a>  -->
							<a href="">[pdf]</a> / [<a href="">code</a>]
              <p></p>
              <p>
                In-context learning (ICL) augments the capabilities of large language models (LLMs) in various downstream tasks by leveraging input and output exemplars. This paper explores the use of translation memory (TM) as a form of prompting to aid LLMs in machine translation tasks. Notably, the LLM's inherent ability to comprehend these prompts significantly bolsters the use of TM. Experimental results indicate that incorporating TM considerably enhances the translation proficiency of the LLM, elevating its BLEU score to levels commensurate with state-of-the-art neural machine translation systems.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/ACL2023_manager.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning</papertitle>
              <!-- </a> -->
              <br>
              Xiao Xu, <strong>Bei Li</strong>, Chenfei Wu, Shao-Yen Tseng, Anahita Bhiwandiwalla, Shachar Rosenman, Vasudev Lal, Wanxiang Che and Nan Duan
              <br>
              <em>61th Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, 2023 
              <br>
							<!-- <a href="#">[pdf](not aviable)</a> / 
							<a href="#">[code](not aviable)</a>  -->
							<a href="">[pdf]</a> / [<a href="">code</a>]
              <p></p>
              <p>
                we propose ManagerTower, a novel Vision-Language model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. ManagerTower outperforms prior work
              </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/ACL2023.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>TranSFormer: Slow-Fast Transformer for Machine Translation</papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>,
              Yi Jing, Xu Tan, Tong Xiao and Jingbo Zhu
              <br>
              <em>61th Annual Meeting of the Association for Computational Linguistics (<strong>Findings of ACL</strong>)</em>, 2023 
              <br>
							<!-- <a href="#">[pdf](not aviable)</a> / 
							<a href="#">[code](not aviable)</a>  -->
							<a href="">[pdf]</a> / [<a href="">code</a>]
              <p></p>
              <p>
                Building upon our previous ICML work, we refine the extraction of fine-grained character-level features by developing a multiscale Transformer model with a two-branch architecture. The Slow-Fast framework effectively mitigates the computational overhead associated with capturing long-term dependencies among character-level sequences, while employing a cross-granularity attention mechanism to learn interactions between the fast and slow branches. Comprehensive experiments conducted on multiple machine translation benchmarks attest to the efficacy of our proposed TranSFormer model.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/umst.png' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Learning Multiscale Transformer Models for Sequence Generation</papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>,
              Tong Zheng, Yi Jing, Chengbo Jiao, Tong Xiaoand Jingbo Zhu
              <br>
              <em>International Conference on Machine Learning (<strong>ICML, Spotlight</strong>)</em>, 2022  
              <br>
							<!-- <a href="#">[pdf](not aviable)</a> / 
							<a href="#">[code](not aviable)</a>  -->
							<a href="https://proceedings.mlr.press/v162/li22ac/li22ac.pdf">[pdf]</a> / [<a href="https://github.com/libeineu/UMST">code</a>]
              <p></p>
              <p>We re-define the concept of scale for NLP, including scales of sub-word, word and phrase. Our intention is to leverage the word boundaries and phrase-level prior knowledge to compensate for the sub-word features. Then we establish the relationships among different scales, resulting in builting a multiscale Transformer model.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/multimodal.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>On Vision Features in Multimodal Machine Translation</papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>,
              Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong Xiao, Anxiang Ma and Jingbo Zhu
              <br>
              <em>60th Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, 2022  
              <br>
							<a href="https://aclanthology.org/2022.acl-long.438.pdf">[pdf]</a> / [<a href="https://github.com/libeineu/fairseq_mmt">code</a>]
							<!-- <a href="#">[code](not aviable)</a>  -->
              <p></p>
              <p> This work investigates the effect of vision features in multimodal machine translation (MMT) scenarios. We proposed three probing tasks to evaluate MMT systems which can help the following researchers. The main contribution is to reveal the importance of strong vision features.</p>
            </td>
          </tr>
				
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/ode_transformer.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Quan Du, Tao Zhou, Yi Jing, Shuhan Zhou, Xin Zeng, Tong Xiao,and Jingbo Zhu
              <br>
              <em>60th Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, 2022  
              <br> 
              [<a href="https://aclanthology.org/2022.acl-long.571.pdf">pdf</a>] / [<a href="https://github.com/libeineu/ODE-Transformer">code</a>]
              <p></p>
              <p>This work attempts to further enhance the standard sequence-level KD method by taking full advantage of the teacher parameters and generate the parameters for student.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/acl2021.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Weight Distillation: Transferring the Knowledge in Neural Network Parameters
                  </papertitle>
              <!-- </a> -->
              <br> 
              Ye Lin, Yanyang Li, Ziyang Wang, <strong>Bei Li</strong>, Quan Du, Tong Xiao, Jingbo Zhu
              <br>
              <em>59th Annual Meeting of the Association for Computational Linguistics (<strong>ACL, Oral</strong>)</em>, 2021  
              <br> 
              [<a href="https://aclanthology.org/2021.acl-long.162.pdf">pdf</a>] / [code]
              <p></p>
              <p>This work establishes the relationship between ODE and the design of Transformer architecture. We also redesign the Transformer architecture inspired by the lower truncation error achieved by high-order solvers in ODE. ODE Transformer can deliver much better translation performance within the same model capacity. Experimental results on three sequence generation tasks demonstrate the effectiveness.
              </p>
            </td>
          </tr>
 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/GPKD.png' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Learning Light-Weight Translation Models from Deep Transformer
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Ziyang Wang, Hui Liu, Quan Du, Tong Xiao, Chunliang Zhang, Jingbo Zhu
              <br>
              <em>Thirty-Fifth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2021  
              <br> 
              [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17561">pdf</a>] / [<a href="https://github.com/libeineu/GPKD">code</a>]
              <p></p>
              <p>This work attempts to learn a light-weight translation model from a deep Transformer teacher network. It introduces a group-permutation based knowledge distillation method to compressing a strong deep Transformer teacher into a much shallower counterpart with a minor BLEU degradation. Furthermore, to enhance the performance of the teacher network, we also propose a skipping sub-layer regularization training method to randomly omit some sub-layers vertically. Both methods can be well applicable into the teacher training process.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/sdt.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Shallow-to-Deep Training for Neural Machine Translation
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, Jingbo Zhu
              <br>
              <em>The 2020 Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>)</em>, 2020  
              <br> 
              [<a href="https://aclanthology.org/2020.emnlp-main.72.pdf">pdf</a>] / [<a href="https://github.com/libeineu/SDT-Training">code</a>]
              <p></p>
              <p>Deep Transformer systems have been widely investigated in the MT community recently. However, with the model going deeper, a crucial challenge is the huge memory cost and extremely long training time. We investigate the behavior of trained systems and find that adjacent layers behave similarly. Thus, we proposed a shallow-to-deep training method instead of learning from scratch which speeds up the training process up to 1.5 times with no loss in BLEU.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/context-aware.png' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation
                  </papertitle>
              <!-- </a> -->
              <br> 
              <strong>Bei Li</strong>,
              Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li
              <br>
              <em>58th Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, 2020  
              <br> 
              [<a href="https://www.aclweb.org/anthology/2020.acl-main.322.pdf">pdf</a>] / [<a href="https://github.com/libeineu/Context-Aware">code</a>]
              <p></p>
              <p>We investigate a general-used multi-encoder framework on document-level machine translation task. It utilizes an additional context-encoder to capture the relationship between the current sentence and its contextual information. However, through specially designed context inputs, we find that the context-encoder acts more like a noise generator instead of encoding the contextual information, which is similar with dropout.Especially when we turn off the context-encoder during inference, there is even slight improvements in terms of BLEU score.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/dlcl.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>Learning deep transformer models for machine translation
                  </papertitle>
              <!-- </a> -->
              <br> 
              Qiang Wang,
              <strong>Bei Li</strong>,
              Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, Lidia S Chao 
              <br>
              <em>57th Annual Meeting of the Association for Computational Linguistics (<strong>ACL, Oral</strong>)</em>, 2019  
              <br> 
              [<a href="https://aclanthology.org/P19-1176.pdf">pdf</a>] / [<a href="https://github.com/wangqiangneu/dlcl">code</a>]
              <p></p>
              <p>It studies deep encoders in Transformer and mathematically explains the importance of the location of layer normalization for deep models. It also proposes a novel connection schema to successfully train a 30-layer Transformer system, which is the deepest encoder at that time. While, it is one of the most high cited NMT papers.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/wmt19.jpg' width="160" mar>
              </div>
 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="#"> -->
                <papertitle>The niutrans machine translation systems for wmt19
                  </papertitle>
              <!-- </a> -->
              <br>
              <strong>Bei Li</strong>,
              Yinqiao Li, Chen Xu, Ye Lin, Jiqiang Liu, Hui Liu, Ziyang Wang, Yuhao Zhang, Nuo Xu, Zeyang Wang, Kai Feng, Hexuan Chen, Tengbo Liu, Yanyang Li, Qiang Wang, Tong Xiao, Jingbo Zhu
              <br>
              <em>Fourth Conference on Machine Translation (<strong>WMT, Workshop of ACL</strong>)</em>, 2019 
              <br>
							<a href="https://aclanthology.org/W19-5325.pdf">[pdf]</a> / 
							<!-- <a href="#">[code](not aviable)</a>  -->
              [code]
              <p></p>
              <p>It describes the submission of the NiuTrans systems for WMT2019 on both supervised and unsupervised tasks, including 13 language directions. This paper shows the details about model architectures, data augmentation methods, ensemble knowledge distillation and system combination strategies.
              </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Honors & Awards</heading>
            <ul style="list-style-type:none">
              <p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">

              <li style="margin-bottom:5px">
                National Scholarship (PH.D). <div style="float:right; text-align:right">2022</div>
              </li>
              
              <li style="margin-bottom:5px">
                Top Ten Graduate students of Northeastern University (The May 4th medal). <div style="float:right; text-align:right">2022</div>
              </li>

              <li style="margin-bottom:5px">
                National Scholarship (PH.D). <div style="float:right; text-align:right">2021</div>
              </li>

              <li style="margin-bottom:5px">
                Outstanding Reviewers of EMNLP2021. <div style="float:right; text-align:right">2021</div>
              </li>

              <li style="margin-bottom:5px">
                1st Rank in Chinese-English in terms of human-evaluation on WMT21. <div style="float:right; text-align:right">2021</div>
              </li>

              <li style="margin-bottom:5px">
                The Excellent Master thesis of Liaoning Province. <div style="float:right; text-align:right">2020</div>
              </li>

              <li style="margin-bottom:5px">
                The Excellent Master Graduate of Liao Ning Province. <div style="float:right; text-align:right">2020</div>
              </li>

              <li style="margin-bottom:5px">
                The Excellent Master Graduate of Northeastern. <div style="float:right; text-align:right">2020</div>
              </li>

              <li style="margin-bottom:5px">
                1st Rank in Japanese-English news translation in terms of human-evaluation on WMT20. <div style="float:right; text-align:right">2020</div>
              </li>
              
              <li style="margin-bottom:5px">
                National Scholarship (Master). <div style="float:right; text-align:right">2019</div>
              </li>

              <li style="margin-bottom:5px">
                1st Rank in 3 news translation in terms of auto-evaluation on WMT19. <div style="float:right; text-align:right">2019</div>
              </li>

              <li style="margin-bottom:5px">
                2nd Rank in 3 news translation in terms of auto-evaluation on WMT19. <div style="float:right; text-align:right">2019</div>
              </li>

              <li style="margin-bottom:5px">
                National Scholarship (Master, Rank 1/230). <div style="float:right; text-align:right">2018</div>
              </li>
              
              <li style="margin-bottom:5px">
                The Excellent Graduate of Shenyang. <div style="float:right; text-align:right">2018</div>
              </li>
              
              <li style="margin-bottom:5px">
                1st Rank in Chinese-English news translation in terms of human-evaluation on WMT18. <div style="float:right; text-align:right">2018</div>
              </li>
              <li style="margin-bottom:5px">
                2nd Rank in English-Chinese news translation in terms of auto-evaluation on WMT18. <div style="float:right; text-align:right">2018</div>
              </li>


              <!-- <li style="margin-bottom:5px"><a href="https://challenge.ai.mgtv.com/home">MangoTV</a> International audio and video algorithm competition <strong> (Rank 6/193)</strong>. <div style="float:right; text-align:right">2021</div> -->
              </li>
              <br>
              </p>
              </ul>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <heading>Intern Experiences</heading>
          <tr>
            <td  style="padding:20px;width:35%;vertical-align:middle">
              <img width="160" src="./images/weiruan.png">
            </td>
            <td style="margin-left:20px;width:65%;vertical-align:middle">
              <div >
                Research Intern, MicroSoft Research Asia, Natural Language Computing
              </div>
              Mar. 2022 - Dec. 2022 <br>
              Advisor: Chenfei Wu <br>
              Text-to-Image Generation, Diffusion Models, Multimodal Modeling
            </td>
          </tr>

          <tr>
            <td  style="padding:20px;width:35%;vertical-align:middle">
              <img width="160" src="./images/weiruan.png">
            </td>
            <td style="margin-left:20px;width:65%;vertical-align:middle">
              <div >
                Research Intern, MicroSoft Research Asia, Machine Learning
              </div>
              Dec. 2022 - Present <br>
              Advisor: Xu Tan <br>
              Machine Translation, Ordinary Differential Equation, Diffusion Models
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Professional activities</heading>
            <ul style="list-style-type:none">
              <p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">
              <li >
                Conference Reviewer for ACL, EMNLP, ICML, Neurips, AAAI, IJCAI, NAACL, COLING, EACL
              </li>
              <br>
              </p>
              </ul>

        </tbody></table>

				
     
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
             
                <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &copy; Bei Li | Last updated: May. 2023.</p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
